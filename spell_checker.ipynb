{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auCYJXLQMsiU"
      },
      "source": [
        "# packages and dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ms4fbOM8475e",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "\n",
        "#!python -m spacy download nb_core_news_sm \n",
        "!spacy download nb_core_news_sm \n",
        "!pip install transformers\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import json\n",
        "import spacy\n",
        "from spacy.lang.nb.examples import sentences\n",
        "\n",
        "from collections import Counter\n",
        "import copy\n",
        "import pickle\n",
        "\n",
        "import transformers\n",
        "from transformers import pipeline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQkhgrTwtlec"
      },
      "source": [
        "# spell checker and corrector\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXfwNcb3yiYI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec0e8842-40fc-4aef-abe8-47386b5368d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at ltgoslo/norbert2 were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ojbb', 'skatekort']\n",
            "Mener du:\n",
            "når du ha jobb måtte du ha en skattekort\n"
          ]
        }
      ],
      "source": [
        "\n",
        "class spell_checker:\n",
        "\n",
        "  def __init__(self,model_path):\n",
        "    \"\"\"\n",
        "    overview: The class is a representation for a spell checker and auto-corrector for Norwegian. \n",
        "    We built it from scrtach for Norwegian language (inspired by levenshtein distance algo.) and powered by AI.  \n",
        "    Arg: a path of the  model, more spesific for BERT which is compatible with mlm-task.\n",
        "    Returns: a list of tokenized words\n",
        "    \"\"\"\n",
        "\n",
        "    self.unmasker = pipeline('fill-mask', model= model_path)\n",
        "\n",
        "    # a Norwegian dic\n",
        "    self.dic=self.read_dic() \n",
        "\n",
        "    # the Norwegian alphabet\n",
        "    self.alphabet=\"a b c d e f g h i j k l m n o p q r s t u v w x y z æ ø å\".split()\n",
        "\n",
        "  def tokenize(self, sent:str)->list:\n",
        "    \"\"\"\n",
        "    overview: It splits a given doc/sentence, written in norwegian, into words.\n",
        "    Arg: a doc/sentence\n",
        "    Returns: list of tokenized words\n",
        "    \"\"\"\n",
        "    return nltk.word_tokenize(sent)\n",
        " \n",
        "\n",
        "  def lemmatize(self, doc:str)->list:  \n",
        "    \"\"\"\n",
        "    overview: a Norwegian lemmatizer which provides the base form of a given sentence/doc.\n",
        "    Arg: a doc/sentence\n",
        "    Returns: list of lemmas (=base form of the words)\n",
        "    \"\"\"\n",
        "    no_lemmatizer = spacy.load(\"nb_core_news_sm\") \n",
        "\n",
        "    p_marks=string.punctuation\n",
        "      \n",
        "    lemmas = [] \n",
        "      \n",
        "    for w in doc:\n",
        "      if w.strip() not in p_marks:\n",
        "        lemma=re.sub(\"[0-9]+\", \"\",w.lower())\n",
        "        lemma=(no_lemmatizer(w)[0].lemma_).lower()\n",
        "        lemmas.append(lemma)\n",
        "    return lemmas\n",
        "\n",
        "\n",
        "  def read_dic(self)->list:\n",
        "    \"\"\"\n",
        "    overview: It reads the Norwegian dictionary file.\n",
        "    Returns: list of the dic words \n",
        "    \"\"\"\n",
        "    f=open(\"dic.txt\",\"r\", encoding='utf-8')\n",
        "    dic=[]\n",
        "    for line in f:\n",
        "        l=line.split(\"\\n\")\n",
        "        dic.append(l[0].lower())\n",
        "    f.close()\n",
        "    return dic\n",
        "\n",
        "\n",
        "  def find_misspelled_ws(self,user_inp:str)->list:\n",
        "    \"\"\"\n",
        "    overview: it detects the misspelled words in a search text by using the dic.\n",
        "    arg: search text given by the user\n",
        "    returns: a list of misspelled words of the inp (if found)\n",
        "    \"\"\"\n",
        "    mis_ws=[]\n",
        "    \n",
        "    for w in user_inp:\n",
        "      if w.lower() not in self.dic:\n",
        "        mis_ws.append(w.lower())\n",
        "    return mis_ws\n",
        "\n",
        "\n",
        "  def insert(self,word:str)->list:\n",
        "    \"\"\"\n",
        "    overview: it generates strings by adding Norwegians letters (one letter by time) before and after every \n",
        "    existing letter of the misspelled word.\n",
        "    arg: a misspelled word as a String\n",
        "    returns: a list of the generated strings by adding \n",
        "    \"\"\"\n",
        "    ar=[]\n",
        "    chars=list(word)\n",
        "    \n",
        "    suggested_strs=[]\n",
        "    for i,char in enumerate(chars): \n",
        "      for l in self.alphabet:\n",
        "        if i==len(chars)-1:\n",
        "          ar.append( word[:i+1] + l)\n",
        "        else:\n",
        "          ar.append( word[:i] +l+ word[i:])\n",
        "    return ar\n",
        "\n",
        "\n",
        "  def delete(self, word:str)->list:\n",
        "    \"\"\"\n",
        "    overview: it generates strings by recursively deleting an existing letter of the misspelled word.\n",
        "    arg: a misspelled word as a String\n",
        "    returns: a list of the generated strings\n",
        "    \"\"\"\n",
        "    generated_strs=[]\n",
        "    chars=list(word)\n",
        "    #if the nr. self.alphabet is 2 then no need to delete.\n",
        "    if len(word)<=2:\n",
        "      return []\n",
        "\n",
        "    for i in range(len(chars)): \n",
        "        temp=word[:]        \n",
        "        res_str = temp.replace(temp[i], '', 1)\n",
        "        generated_strs.append(res_str)\n",
        "        \n",
        "    return generated_strs\n",
        "\n",
        "\n",
        "  def replace(self, word:str)->list:\n",
        "    \"\"\"\n",
        "    overview: it generates strings by recursively replacing an existing letter of the misspelled word with an another.\n",
        "    arg: a misspelled word as a String\n",
        "    returns: a list of the generated strings\n",
        "    \"\"\"\n",
        "    generated_strs=[]\n",
        "    chars=list(word)  \n",
        "    for i, char in enumerate(chars): \n",
        "      temp=chars[i]\n",
        "      for  l in self.alphabet: \n",
        "        chars[i]=l\n",
        "        replaced_w=\"\".join(chars)\n",
        "        generated_strs.append(replaced_w)\n",
        "      chars[i]=temp\n",
        "    return generated_strs\n",
        "\n",
        "\n",
        "  def swap(self, word:str)->list:\n",
        "    \"\"\"\n",
        "    overview: it generates strings by recursively swapping two and two letters of the misspelled word.\n",
        "    arg: a misspelled word as a String\n",
        "    returns: a list of the generated strings\n",
        "    \"\"\"\n",
        "    generated_strs=[]\n",
        "    chars=list(word) \n",
        "    if len(chars)>1:\n",
        "      for i in range(len(chars)-1):\n",
        "        temp=chars.copy()\n",
        "        t=temp[i]\n",
        "        temp[i]= temp[i+1]\n",
        "        temp[i+1]= t\n",
        "        generated_strs.append(\"\".join(temp))\n",
        "      return generated_strs\n",
        "\n",
        "\n",
        "  def check_one(self, word:str)->list:\n",
        "    \"\"\"\n",
        "    overview: It just collects all generates strings given by the previous four methods.\n",
        "    arg: a misspelled word as a String\n",
        "    returns: a list of all generated strings\n",
        "    \"\"\"\n",
        "    \n",
        "    gen_strs=self.insert(word), self.delete(word), self.replace(word),self.swap(word)\n",
        "    concat_list=[]\n",
        "    for lst in gen_strs:\n",
        "      concat_list.extend(lst)\n",
        "    return sorted(set(concat_list))\n",
        "\n",
        "  \n",
        "  def check_all(self,miss_s_ws:list)->list:\n",
        "    \"\"\"\n",
        "    overview: It just collects all generates strings given by the previous four methods for ALL misspelled words.\n",
        "    arg: a list of the misspelled word as Strings\n",
        "    returns: a matrix of all generated strings fr all misspelled words of the user input.\n",
        "    \"\"\"\n",
        "    return [self.check_one(w) for w in miss_s_ws]\n",
        "\n",
        "\n",
        "  def wrds_frm_strs(self, strs:list)->list:\n",
        "    \"\"\"\n",
        "    overview: It extracts the words from the given \n",
        "    arg: a list of the misspelled word as Strings\n",
        "    returns: a matrix of all generated strings fr all misspelled words of the user input.\n",
        "    \"\"\"\n",
        "    words=[]\n",
        "    for w in strs:\n",
        "      if w  in self.dic:\n",
        "        words.append(w)  \n",
        "    return words\n",
        "     \n",
        "\n",
        "  def mask(self, miss_s_ws:list,user_inp:str)->str:\n",
        "    \"\"\"\n",
        "    Returns: \n",
        "      A masked string which our model need it to predict a list of suggested words/tokens.\n",
        "      Se method unmask_and_suggest_ws()\n",
        "    Args:\n",
        "      A list of the missspelled words\n",
        "    \"\"\"\n",
        "    user_inp_copy=user_inp[:]\n",
        "    \n",
        "    print(miss_s_ws)\n",
        "    for e in miss_s_ws:\n",
        "      user_inp_copy[user_inp_copy.index(e)]='[MASK]' \n",
        "      concat=' '.join(user_inp_copy)\n",
        "    return concat\n",
        "\n",
        "  def unmask_and_suggest_ws(self, miss_s_ws:list,user_inp:str)->list:\n",
        "    \"\"\"\n",
        "    Overview:\n",
        "      By using AI, it masks the misspelled words and suggest candidates \n",
        "      words of them based on context of the user input.\n",
        "    Args:\n",
        "      miss_s_ws: a list of the misspelled words\n",
        "      user_inp: user input/search txt as a string\n",
        "    Returns: \n",
        "      a list of lists where every list represents a set of the candidate words of a given misspelled word. \n",
        "     \"\"\"\n",
        "    masked_inp=self.mask(miss_s_ws,user_inp)\n",
        "    unmasked=self.unmasker(masked_inp)\n",
        "    if len(unmasked)==0 or not unmasked:\n",
        "      raise Exception(\"An error comes from the model!\")\n",
        "\n",
        "    suggested_words=[]\n",
        "\n",
        "    if len(miss_s_ws)==1:  \n",
        "      #candidate words for one misspelled word\n",
        "      temp=[unmasked[i]['token_str'].lower() for i in range(len(unmasked))]\n",
        "      suggested_words.append(temp)\n",
        "    else:\n",
        "      for l in unmasked:\n",
        "        #candidate words for one misspelled word\n",
        "        temp= [l[i]['token_str'].lower() for i in range(len(l))] \n",
        "        suggested_words.append(temp)\n",
        "    return suggested_words\n",
        "\n",
        "\n",
        "  def find_best_candidate(self, extracted_ws:list, suggested_ws:list)->str: \n",
        "    \"\"\"\n",
        "    Overview:\n",
        "     Given a list of the  words whcih are extracted from the generated strings and a list of\n",
        "     the words/tokens which are suggested/predicted by mlm model, the method tries to find a match bewteen them.\n",
        "     In other words, the method sees if one of the predicted words is in the extracted words to replace the misssepelled word.\n",
        "    returns:\n",
        "      a matched token if found otherwise an empty string ''\n",
        "      str: user input/search text\n",
        "    Args:\n",
        "      str list: extracted from the generated strings\n",
        "      str list: suggested candidate words\n",
        "    \"\"\"\n",
        "    for w in suggested_ws:\n",
        "      if w in extracted_ws:\n",
        "        return w\n",
        "    return ''\n",
        "\n",
        "\n",
        "  def read_json(self, file)->list:\n",
        "    \"\"\"\n",
        "    overview: It reads the frequecy table as a json file\n",
        "    arg: a json file\n",
        "    returns: a list of the items .. (words : their frequency)\n",
        "    \"\"\"\n",
        "    f = open(file) \n",
        "    data = json.load(f)  \n",
        "    return data\n",
        "\n",
        "\n",
        "  def find_best_freq_table(self, extracted_wrds:list, freq_table:list)->str:\n",
        "    \"\"\"\n",
        "    overview: It find the most frequent candidate out of a set of candidate\n",
        "    to replace the misspelled word.\n",
        "    arg: a list of the candidates for a misspelled word\n",
        "    returns: the most most frequent candidate\n",
        "    \"\"\"\n",
        "    temp_freq={}\n",
        "    for w in extracted_wrds:\n",
        "      if w not in freq_table:\n",
        "        temp_freq[w]=1\n",
        "      else:\n",
        "        temp_freq[w]=freq_table[w]\n",
        "    return max(temp_freq, key=temp_freq.get)\n",
        "\n",
        "\n",
        "\n",
        "  def auto_correct(self, search_txt:str):\n",
        "    \"\"\"\n",
        "    overview: It is the main function which puts all other functions together.\n",
        "    arg: search_txt or user input as a string\n",
        "    returns: a string of the corrected input or the same user \n",
        "    input if no misspelled word or no candidate word for the misspelled word\n",
        "    \"\"\"\n",
        "    user_inp=self.lemmatize(self.tokenize(search_txt))   #a list of cleaned input\n",
        "    user_inp_copy = user_inp[:]\n",
        "    \n",
        "    miss_s_ws=self.find_misspelled_ws(user_inp)\n",
        "    \n",
        "    if len(miss_s_ws)==0:\n",
        "      #no misspelled words in the user input \n",
        "      return user_inp\n",
        "\n",
        "    else: \n",
        "      gen_strings=self.check_all(miss_s_ws)\n",
        "      \n",
        "      extracted_wrds=[self.wrds_frm_strs(strs) for strs in gen_strings]\n",
        "\n",
        "      if len(extracted_wrds)==0:\n",
        "        # no generated string is a word \n",
        "        return user_inp\n",
        "\n",
        "      #the final candidates list which will replace the misspelled word(s) if found\n",
        "      chosen=[]\n",
        "      \n",
        "      suggested_candidates=self.unmask_and_suggest_ws(miss_s_ws, user_inp_copy)\n",
        "\n",
        "      for i, l in enumerate(suggested_candidates):\n",
        "        \n",
        "        #best candidate if found\n",
        "        best_candidate_mlm=self.find_best_candidate(extracted_wrds, l)\n",
        "        \n",
        "        if len(best_candidate_mlm)!=0:\n",
        "          chosen.append(best_candidate_mlm)\n",
        "        else:\n",
        "          ex_wrds=extracted_wrds[i][:]\n",
        "          freq_table=self.read_json(\"freq_table.json\")\n",
        "          best_candidate_freqTable=self.find_best_freq_table(ex_wrds,freq_table)\n",
        "          chosen.append(best_candidate_freqTable)\n",
        "      \n",
        "      temp=[]\n",
        "      user_inp_copy2=user_inp[:]\n",
        "      for i, candidate in enumerate(chosen):\n",
        "        ii =user_inp.index(miss_s_ws[i]) #index of the misspelled word\n",
        "        user_inp_copy2[ii]=candidate \n",
        "      \n",
        "      print(\"Mener du:\")\n",
        "      formated_answer=' '.join(user_inp_copy2) \n",
        "    return formated_answer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "  #demo\n",
        "\n",
        "  inp=\"Når du har ojbb må du ha et skatekort.\"\n",
        "  sc=spell_checker('ltgoslo/norbert2')\n",
        "  print(sc.auto_correct(inp))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "output:\n",
        "  Mener du:\n",
        "  når du ha jobb måtte du ha en skattekort\n",
        "\n",
        "\"\"\"\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "auCYJXLQMsiU"
      ],
      "name": "Copy of spell_checker.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}